{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf9qXwnjNKUU"
   },
   "source": [
    "## NLP\n",
    "\n",
    "Natural Language Processing\n",
    "\n",
    "- Advanced - 深度學習 Deep Learning\n",
    "- Basic - 機器學期 Machine Learning </br>\n",
    "\n",
    " Step 1 : 前處理 Pre-Process </br>\n",
    " > 1. 中文分詞 Tokenization\n",
    " > 2. 詞性標註 Parts of Speech Tagging\n",
    " > 3. 命名實體識別 NER\n",
    " > 4. 去除停用詞\n",
    "\n",
    " Step 2 : 特徵工程 Feature Engineering </br>\n",
    " Step 3 : 機器學習\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lxci640AaNk"
   },
   "source": [
    "# 1.分詞 – Tokenization\n",
    "\n",
    "分詞是 NLP 的基本且重要步驟。</br> 分詞就是將句子、段落、文章這種長文本，分解為以`字詞`為單位的數據結構，方便後續的處理分析工作。\n",
    "\n",
    "- Input\n",
    "```\n",
    "有天小明搭公車，不小心把健保卡當作悠遊卡。健保卡就對小明說： 你為什麼要這樣嗶我？\n",
    "```\n",
    "\n",
    "- Output\n",
    "```\n",
    "有天 小明 搭 公車 不 小心 把 健保卡 當作 悠遊卡 健保卡 就 對 小 明說 你 為什麼 要 這樣 嗶 我\n",
    "```\n",
    "</br>\n",
    "\n",
    "繁中斷詞工具：\n",
    "1. 結巴(Jieba)\n",
    " > https://github.com/fxsjy/jieba\n",
    "2. 中研院的繁體中文斷詞系統 CKIPtagger\n",
    " > https://github.com/ckiplab/ckiptagger/wiki/Chinese-README\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD_a2ge5IH50"
   },
   "source": [
    "## jieba\n",
    "1. 基本斷詞用法：預設詞庫\n",
    "2. 進階：自定詞庫\n",
    "\n",
    "#### 特點\n",
    "支持四种分词模式：\n",
    "1. 精確模式：试图将句子最精确地切开，适合文本分析。預設精確模式\n",
    "2. 全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n",
    "3. 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "4. paddle模式：利用 PaddlePaddle 深度学习框架，训练序列标注（双向GRU）网络模型实现分词。同时支持词性标注\n",
    "\n",
    "Reference : \n",
    "> [如何使用 jieba 結巴中文分詞程式](https://blog.fukuball.com/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-jieba-%E7%B5%90%E5%B7%B4%E4%B8%AD%E6%96%87%E5%88%86%E8%A9%9E%E7%A8%8B%E5%BC%8F/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1622961846566,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "mMsFa0ZHLikk"
   },
   "outputs": [],
   "source": [
    "#@title import library { form-width: \"150px\"}\n",
    "\n",
    "# !pip install jieba\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taOszW5NeJZl"
   },
   "source": [
    "## jieba.cut\n",
    "\n",
    "- output token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1622961860407,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "QUFEse9mcpmI",
    "outputId": "b84c9742-b65b-49be-eb57-57fe2b96e77a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x7fba35e2edd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"日本疫苗及時雨！101、圓山飯店點燈感謝「千里送暖 友誼長存」\"\n",
    "jieba.cut(sentence)\n",
    "\n",
    "# output : Tokenizer.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 950,
     "status": "ok",
     "timestamp": 1622951765923,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "jMLIUUpYdTSn",
    "outputId": "6d53a88e-c0c7-415a-a0b1-2fe3d4d6064d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.104 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本,疫苗,及,時雨,！,101,、,圓山飯,店點,燈,感謝,「,千里,送暖, ,友誼長,存,」\n"
     ]
    }
   ],
   "source": [
    "# use .join to concat words\n",
    "output = jieba.cut(sentence)\n",
    "\n",
    "print( ','.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M6PjeFhePIP"
   },
   "source": [
    "## jieba.lcut\n",
    "\n",
    "- output 形式為 list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1622951765924,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "X1MkN2qdczmt",
    "outputId": "c7c5e2ab-4c8a-4086-e44f-14aaa517e9e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['日本',\n",
       " '疫苗',\n",
       " '及',\n",
       " '時雨',\n",
       " '！',\n",
       " '101',\n",
       " '、',\n",
       " '圓山飯',\n",
       " '店點',\n",
       " '燈',\n",
       " '感謝',\n",
       " '「',\n",
       " '千里',\n",
       " '送暖',\n",
       " ' ',\n",
       " '友誼長',\n",
       " '存',\n",
       " '」']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"日本疫苗及時雨！101、圓山飯店點燈感謝「千里送暖 友誼長存」\"\n",
    "\n",
    "jieba.lcut(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovRsIZBVenUw"
   },
   "source": [
    "### 預設詞庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13656,
     "status": "ok",
     "timestamp": 1622951779575,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "F95WoWunQEnB",
    "outputId": "60372ea2-760f-4fb7-debc-9cdf5e6c9d4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing paddle-tiny, please wait a minute......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【精確模式】　 ['日本', '疫苗', '及', '時雨', '！', '101', '、', '圓山飯', '店點', '燈', '感謝', '「', '千里', '送暖', ' ', '友誼長', '存', '」']\n",
      "【偽精確模式】 ['日本', '疫苗', '及', '時', '雨', '！', '101', '、', '圓', '山', '飯', '店', '點', '燈', '感', '謝', '「', '千里', '送暖', ' ', '友', '誼', '長', '存', '」']\n",
      "【全模式】　　 ['日本', '疫苗', '及', '時', '雨', '！', '101', '、', '圓', '山', '飯', '店', '點', '燈', '感', '謝', '「', '千里', '送暖', '', ' ', '', '友', '誼', '長', '存', '」']\n",
      "【搜索引擎】　 ['日本', '疫苗', '及', '時雨', '！', '101', '、', '圓山飯', '店點', '燈', '感謝', '「', '千里', '送暖', ' ', '友誼長', '存', '」']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【paddle】 　 ['日本', '疫苗', '及', '時雨！101、圓山飯店點燈感謝「千里送暖 友誼長', '存', '」']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"日本疫苗及時雨！101、圓山飯店點燈感謝「千里送暖 友誼長存」\"\n",
    "\n",
    "\n",
    "# **預設精確：精確模式並套用 HMM：使用隱藏式馬可夫鍊（HMM）模型，試圖找出不在字典檔裡面的字。\n",
    "seg_list = jieba.lcut(sentence)\n",
    "print('【精確模式】　', seg_list )\n",
    "\n",
    "\n",
    "# 精確：關閉隱藏式馬可夫鍊（HMM）模型\n",
    "seg_list = jieba.lcut(sentence, HMM = False)\n",
    "print('【偽精確模式】', seg_list )\n",
    "\n",
    "\n",
    "# 全模式：把句子中可以成詞的詞語都掃瞄出來，速度非常快但不保證有意義。\n",
    "seg_list = jieba.lcut(sentence, cut_all=True)\n",
    "print('【全模式】　　', seg_list )\n",
    "\n",
    "\n",
    "# 搜索引擎模式：可能的詞再次切分\n",
    "seg_list = jieba.lcut_for_search(sentence)\n",
    "print('【搜索引擎】　' , seg_list)\n",
    "\n",
    "\n",
    "# Paddle：\n",
    "jieba.enable_paddle()\n",
    "\n",
    "seg_list = jieba.lcut(sentence, use_paddle=True)\n",
    "print('【paddle】 　', seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE_RdQFbfTa0"
   },
   "source": [
    "### 繁體詞庫\n",
    "\n",
    "可以自行添加自訂義的詞句，以便辨識 jieba 預設詞庫裡沒有詞 。雖然 jieba 能識別新的詞彙，但自行新增詞彙可以保證更高的正確率\n",
    "\n",
    "``` python \n",
    "jieba.load_userdict(file_path) \n",
    "```\n",
    "1. file_path 須為儲存在 local 端的 txt 檔案\n",
    "> 繁中詞彙：https://raw.githubusercontent.com/fxsjy/jieba/master/extra_dict/dict.txt.big\n",
    "\n",
    "2. 格式：一個詞占一行；每一行分三部分：詞語、頻率（可省略）、詞性（可省略），空格分隔，順序不可改變\n",
    "> 墓誌銘 67 n </br>\n",
    "> 座右銘 54 nr </br>\n",
    "> 有氧 95 vn </br>\n",
    "> 美麗 3036 ns </br>\n",
    "\n",
    "3. 詞頻省略時使用自動計算的能保證分出該詞的詞頻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21148,
     "status": "ok",
     "timestamp": 1622951800712,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "DuIWqvGXMQGF",
    "outputId": "365631fb-1965-406c-fa3c-5cd6ca323bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#@title set_dictionary { form-width: \"150px\"}\n",
    "# 授權 colab 能存取用戶的 google drive 檔案\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# 讀 dict.txt 檔案\n",
    "file_path = '/content/drive/MyDrive/Colab Notebooks/jieba/dict.txt'\n",
    "jieba.set_dictionary(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2723,
     "status": "ok",
     "timestamp": 1622951803426,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "-WASMEqujlsV",
    "outputId": "3c772cab-7cd3-4a14-8ccf-d67002d12456"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /content/drive/MyDrive/Colab Notebooks/jieba/dict.txt ...\n",
      "Dumping model to file cache /tmp/jieba.u4e0deaaf5fdff1da756b309039a2d83e.cache\n",
      "Loading model cost 2.434 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['爾', '俸爾祿', '，', '民膏民脂', '，', '下民易虐', '，', '上天', '難', '欺', '，', '遲刻', '魔', '，', '藍瘦', '香菇', '，', '哈利波', '特']\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"日本疫苗及時雨！101、圓山飯店點燈感謝「千里送暖 友誼長存」\"\n",
    "sentence = '爾俸爾祿，民膏民脂，下民易虐，上天難欺，遲刻魔，藍瘦香菇，哈利波特'\n",
    "\n",
    "seg_list = jieba.lcut(sentence)\n",
    "print( seg_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEUdn832lWuk"
   },
   "source": [
    "### 自訂詞庫\n",
    "\n",
    "\n",
    "``` python \n",
    "jieba.load_userdict(file_path) \n",
    "```\n",
    "\n",
    "\n",
    "1. file_path 須為儲存在 local 端的 txt 檔案\n",
    "> 範例：https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big\n",
    "\n",
    "2. 格式：一個詞占一行；每一行分三部分：詞語、頻率（可省略）、詞性（可省略），空格分隔，順序不可改變\n",
    "> 墓誌銘 67 n </br>\n",
    "> 座右銘 54 nr </br>\n",
    "> 有氧 95 vn </br>\n",
    "> 美麗 3036 ns </br>\n",
    "\n",
    "3. 詞頻省略時使用自動計算的能保證分出該詞的詞頻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13909,
     "status": "ok",
     "timestamp": 1622951817331,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "tCMO2cktlh0p"
   },
   "outputs": [],
   "source": [
    "#@title load_userdict { form-width: \"150px\"}\n",
    "file_path = '/content/drive/MyDrive/Colab Notebooks/jieba/userdict.txt'\n",
    "jieba.load_userdict(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1622951817333,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "rgFNZ5oUqBjR",
    "outputId": "630a2be2-29e3-49d7-baac-ff1b73d30b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['爾俸爾祿', '，', '民膏民脂', '，', '下民易虐', '，', '上天難欺', '，', '遲刻魔', '，', '藍瘦香菇', '，', '哈利波特', '，', '港湖', '胖虎', '，', '台', '中']\n"
     ]
    }
   ],
   "source": [
    "sentence = '爾俸爾祿，民膏民脂，下民易虐，上天難欺，遲刻魔，藍瘦香菇，哈利波特，港湖胖虎，台中'\n",
    "\n",
    "seg_list = jieba.lcut(sentence)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1622951817334,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "lvRr1H5CrdH_",
    "outputId": "ba354311-f4e2-4297-c958-02f0fb03f770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['爾俸爾祿', '，', '民膏民脂', '，', '下民易虐', '，', '上天難欺', '，', '遲刻魔', '，', '藍瘦香菇', '，', '哈利波特', '，', '港湖胖虎', '，', '台', '中']\n"
     ]
    }
   ],
   "source": [
    "#@title add_word { form-width: \"150px\"}\n",
    "jieba.add_word('港湖胖虎', freq=None, tag=None)\n",
    "\n",
    "seg_list = jieba.lcut(sentence)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1622951817334,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "AV5e2ErhrseF",
    "outputId": "002fc004-8db8-4881-efd9-6e8a29f791ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['爾俸爾祿', '，', '民膏民脂', '，', '下民易虐', '，', '上天難欺', '，', '遲刻魔', '，', '藍瘦香菇', '，', '哈利波特', '，', '港湖胖虎', '，', '台中']\n"
     ]
    }
   ],
   "source": [
    "#@title suggest_freq { form-width: \"150px\"}\n",
    "jieba.suggest_freq('台中', tune=True)\n",
    "\n",
    "seg_list = jieba.lcut(sentence)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1622951817335,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "ih-OB_xPst7l",
    "outputId": "f8ddc044-35ed-41f0-b736-79a81d98c672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天天氣', '不錯']\n",
      "['今天', '天氣', '不錯']\n"
     ]
    }
   ],
   "source": [
    "#@title del_word { form-width: \"150px\"}\n",
    "word = '今天天氣不錯'\n",
    "print( jieba.lcut(word) )\n",
    "\n",
    "\n",
    "jieba.del_word('今天天氣')\n",
    "print(jieba.lcut(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sbTgE8a7rRy"
   },
   "source": [
    "# 2.詞性標註 Parts of Speech\n",
    "\n",
    "大部份的斷詞系統都可以列出斷詞的詞性，jieba 也有這個功能，但結果可能不是那麼好，這其實是跟所使用的語料庫有關係\n",
    "\n",
    "\n",
    "其中詞性標籤 24 個（小寫字母），專名類別標籤 4 個（大寫字母）\n",
    "\n",
    "標籤|\t詞性\t|標籤\t|詞性\t|標籤\t|詞性|\t標籤\t|詞性\n",
    "----|:----------|:-----:|-----:| ----:|-------|:--------|:---------\n",
    "n\t|普通名词\t|f\t|方位名词\t|s\t|处所名词\t|t\t|时间\n",
    "nr\t|人名\t   |ns\t|地名\t|nt\t|机构名\t|nw\t|作品名\n",
    "nz\t|其他专名\t|v\t|普通动词\t|vd\t|动副词\t|vn\t|名动词\n",
    "a\t|形容词    |ad\t|副形词\t|an\t|名形词\t|d\t|副词\n",
    "m\t|数量词\t   |q     |量词\t|r\t|代词\t|p\t|介词\n",
    "c\t|连词\t  |u\t|助词\t|xc\t|其他虚词\t|w\t|标点符号\n",
    "PER\t|人名\t  |LOC\t|地名|\tORG\t|机构名\t|TIME\t|时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 910,
     "status": "ok",
     "timestamp": 1622951818228,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "HpcL9INy8VbW",
    "outputId": "8fd62667-16a5-4b26-c585-ba12d8ee33ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word =  遲刻魔 ; flag =  n\n",
      "word =    ; flag =  x\n",
      "word =  藍瘦香菇 ; flag =  an\n",
      "word =    ; flag =  x\n",
      "word =  哈利波特 ; flag =  n\n",
      "word =    ; flag =  x\n",
      "word =  港湖胖虎 ; flag =  x\n",
      "word =    ; flag =  x\n",
      "word =  台中 ; flag =  s\n"
     ]
    }
   ],
   "source": [
    "#@title pseg.cut {form-width: \"150px\"}\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "content = '遲刻魔 藍瘦香菇 哈利波特 港湖胖虎 台中'\n",
    "\n",
    "words = pseg.cut(content)\n",
    "\n",
    "for word, flag in words:\n",
    "    print( 'word = ', word ,'; flag = ', flag )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxNbzhikAYrM"
   },
   "source": [
    "#3.取出關鍵詞 \n",
    "\n",
    "jieba 支援 TF-IDF 和 TextRank 兩種找出關鍵詞的功能\n",
    "\n",
    "```python\n",
    "jieba.analyse.extract_tags(content, topK=20, withWeight=True, allowPOS=())\n",
    "```\n",
    "\n",
    "```python\n",
    "jieba.analyse.textrank(content, topK=20, withWeight=True, allowPOS=())\n",
    "```\n",
    "\n",
    "> content：待提取關鍵詞的文本 </br>\n",
    "> topK：返回關鍵詞的數量，重要性從高到低排序 </br>\n",
    "> withWeight：是否同時回傳每個關鍵詞的權重 </br>\n",
    "> allowPOS：詞性過濾，預設為空值，回差全不詞性，若提供則僅回傳符合詞性的關鍵詞 </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1622951818638,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "hgIuTwun3mp0",
    "outputId": "805eb22f-d153-44cb-ef7b-4351880b27b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "規格 1.2583965792526315\n",
      "設定 0.6291982896263157\n",
      "紅色 0.6291982896263157\n",
      "SML 0.6291982896263157\n",
      "顯示 0.6291982896263157\n",
      "預購 0.6291982896263157\n",
      "請問 0.31459914481315787\n",
      "庫存 0.31459914481315787\n",
      "開放 0.31459914481315787\n",
      "有貨 0.31459914481315787\n"
     ]
    }
   ],
   "source": [
    "#@title extract_tags {form-width: \"150px\"}\n",
    "\n",
    "import jieba.analyse\n",
    "\n",
    "content = '請問，設定商品時 若我有多規格比如紅色SML+白色SML那我希望某規格比如紅色S庫存售完後 只有該規格顯示為\"開放預購\" 其他有貨的規格還是正常顯示 有辦法這樣設定嗎?因為我有看了教學文章。但都只有主商品直接全部都變成預購'\n",
    "tags = jieba.analyse.extract_tags(content, 10 , withWeight=True)\n",
    "\n",
    "for word, weight in tags:\n",
    "    print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1622951818639,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "0zU8Ke6AwfNW",
    "outputId": "4cae4510-1b08-47f8-c177-e1eeaf04727a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "規格 1.0\n",
      "紅色 0.761781606128585\n",
      "比如 0.7577944077261477\n",
      "預購 0.6382506757077487\n",
      "商品 0.5947352086339321\n",
      "顯示 0.5876558413535816\n",
      "文章 0.4904912570896341\n",
      "教學 0.4870045813997485\n",
      "設定 0.47083215909522097\n",
      "變成 0.42407684529093725\n"
     ]
    }
   ],
   "source": [
    "#@title textrank {form-width: \"150px\"}\n",
    "\n",
    "import jieba.analyse\n",
    "\n",
    "content = '請問，設定商品時 若我有多規格比如紅色SML+白色SML那我希望某規格比如紅色S庫存售完後 只有該規格顯示為\"開放預購\" 其他有貨的規格還是正常顯示 有辦法這樣設定嗎?因為我有看了教學文章。但都只有主商品直接全部都變成預購'\n",
    "tags = jieba.analyse.textrank(content, 10 , withWeight=True)\n",
    "\n",
    "for word, weight in tags:\n",
    "    print(word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvU3E6gsoZGQ"
   },
   "source": [
    "# 4.去除停用字 - Stopwords\n",
    "\n",
    "``` python \n",
    "jieba.analyse.set_stop_words(file_path)\n",
    "```\n",
    "\n",
    "\n",
    "1. file_path 須為儲存在 local 端的 txt 檔案\n",
    "> 範例：https://github.com/goto456/stopwords\n",
    "\n",
    "2. 格式：一個詞占一行；每一行一個詞語\n",
    "> 我</br>\n",
    "> 的</br>\n",
    "> 所以</br>\n",
    "> 後來 </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1622951818912,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "undtdPnIKxgj",
    "outputId": "aea596ff-937c-4d44-e473-b1150ae1a25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['值此', '疫情', '艱難', '，', '且', '全球', '疫苗', '產能', '供不應求', '之際', '，', '對於', '日本', '即時', '提供', '這批', 'COVID', '-', '19', '疫苗', '，', '給予', '臺灣', '國內', '疫情', '防治', '極大', '幫助', '，', '指揮中心', '謹向', '日本政府', '與', '人民', '表達', '誠摯', '謝忱', '與', '感佩', '。']\n",
      "疫情 0.7032\n",
      "疫苗 0.6305\n",
      "艱難 0.5198\n",
      "產能 0.5198\n",
      "供不應求 0.5198\n",
      "COVID 0.5198\n",
      "19 0.5198\n",
      "臺灣 0.5198\n",
      "國內 0.5198\n",
      "幫助 0.5198\n",
      "指揮中心 0.5198\n",
      "表達 0.5198\n",
      "誠摯 0.5198\n",
      "謝忱 0.5198\n",
      "感佩 0.4741\n",
      "日本政府 0.3645\n",
      "防治 0.3256\n",
      "人民 0.2265\n",
      "日本 0.2177\n",
      "全球 0.2131\n",
      "提供 0.2037\n"
     ]
    }
   ],
   "source": [
    "#@title set_stop_words {form-width: \"150px\"}\n",
    "import jieba.analyse\n",
    "\n",
    "file_path = '/content/drive/MyDrive/Colab Notebooks/jieba/stoped.txt'\n",
    "jieba.analyse.set_stop_words(file_path)\n",
    "\n",
    "\n",
    "content = '值此疫情艱難，且全球疫苗產能供不應求之際，對於日本即時提供這批COVID-19疫苗，給予臺灣國內疫情防治極大幫助，指揮中心謹向日本政府與人民表達誠摯謝忱與感佩。'\n",
    "\n",
    "seg_list = jieba.lcut(content)\n",
    "print(len(seg_list))\n",
    "print(seg_list)\n",
    "\n",
    "keywords = jieba.analyse.extract_tags(content,40, withWeight=True)\n",
    "for word , flag in keywords:\n",
    "    print( word, round( flag ,4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x00llCKKoer"
   },
   "source": [
    "# Use Case\n",
    "\n",
    "- Issue : 部分店家當初沒有填類別，是否能幫他們找出可被分類的商店類別?\n",
    "\n",
    "- Methodology : 利用商品名稱找出比較相似的商店，藉此得知適合的類別 tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKRa5QWaZNS4"
   },
   "source": [
    "**BoW (Bag of Words) 與詞彙數量-文件矩陣**\n",
    "- https://taweihuang.hpd.io/2017/03/01/tfidf/ \n",
    "\n",
    "假設現在有 D 篇文件 (document)，而所有文件中總共使用了 T 個詞彙 (term)，我們就可以將文章轉換成以下類型的矩陣，其中第一欄第一列的「12」代表的是「文件 1」 中出現了12個「文字 1」。如此一來，我們可以用 [12, 0, 3,...,2] 這個向量來代表「文件 1」，同理也可用「文件 D」也可以用 [0,2,8,...,0] 來表示。\n",
    "\n",
    "<img src=\"https://taweihuang.hpd.io/wp-content/uploads/2017/03/%E5%9C%96%E7%89%871.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBHwFB0Kannq"
   },
   "source": [
    "**TF-IDF 演算法**\n",
    "\n",
    "<img src=\"https://1.bp.blogspot.com/-tnzPA6dDtTU/Vw6EWm_PjCI/AAAAAAABDwI/JatHtUJb4fsce9E-Ns5t02_nakFtGrsugCLcB/s1600/%25E8%259E%25A2%25E5%25B9%2595%25E5%25BF%25AB%25E7%2585%25A7%2B2016-04-14%2B%25E4%25B8%258A%25E5%258D%25881.39.07.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TF-IDF 演算法包含了兩個部分：\n",
    "- 詞頻（Term frequency，TF）</br>\n",
    "> 詞頻指的是某一個分詞在該文件中出現的頻率，第 *t* 個詞出現在第 *d* 篇文件的頻率。</br>\n",
    "> 例如，如果文件 1 總共有100個字，而第 1 個字在文件 1 出現的次數是12次，因此 tf value = 12/100\n",
    "- 逆向文件頻率（Inverse Document Frequency，IDF）</br>\n",
    "> 常用字處理：比如說， the 在每個文件中都出現好多次，如此一來總體數字較少的文件的向量就會被 the 這個字所主導，但 the 這個字其實沒什麼特別的意義。\n",
    "\n",
    "\n",
    "<img src=\"https://taweihuang.hpd.io/wp-content/uploads/2017/03/%E5%9C%96%E7%89%871-1.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1622951958163,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "9XhnYSiW3o00"
   },
   "outputs": [],
   "source": [
    "#@title build model { form-width: \"150px\"}\n",
    "\n",
    "def semblance(text, corpus):\n",
    "\n",
    "    # test data 斷詞\n",
    "    cut_words = jieba.cut( text )\n",
    "    if cut_words not in stopwords:\n",
    "        cut_words = ','.join(filter(str.isalnum, cut_words))\n",
    "        result = cut_words.split(',')\n",
    "    dic_text_list = [i for i in result if len(re.sub('[a-zA-Z0-9]', '', i)) >=1 ]\n",
    "\n",
    "    # test data 的 bag_of_word\n",
    "    doc_text_vec = dictionary.doc2bow(dic_text_list)\n",
    "\n",
    "    # training data 裡，每個文件中，每個分詞的 tfidf 值\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "\n",
    "    # 对每个文档，分析要测试文档的相似度\n",
    "    index = similarities.SparseMatrixSimilarity( tfidf[corpus], num_features=len(dictionary.keys()) )\n",
    "\n",
    "    sim = index[tfidf[doc_text_vec]]\n",
    "    # print(sim)\n",
    "\n",
    "    sim_sorted = sorted(enumerate(sim), key=lambda x: -x[1])\n",
    "    return sim_sorted"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMGu6suq0kAoeKZ5di0nhq8",
   "collapsed_sections": [],
   "name": "8. Basic Knowledge of NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
