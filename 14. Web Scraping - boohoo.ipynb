{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 練習爬電商的商品名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s_9Qy1GmDZio"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.boohoo.com/womens/sale?start=1&sz=60 1\n",
      "https://www.boohoo.com/womens/sale?start=61&sz=120 2\n",
      "https://www.boohoo.com/womens/sale?start=121&sz=180 3\n",
      "It took 10.511088 sec\n"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/python\n",
    "# coding:utf-8\n",
    "\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import time\n",
    "\n",
    "def sleeptime(hour,min,sec):\n",
    "    return hour*3600 + min*60 + sec;\n",
    "\n",
    "\n",
    "tStart = time.time()#計時開始\n",
    "fp = io.open(\"boohoo_list.txt\", \"ab+\")\n",
    "i = 1\n",
    "while (i<=3):\n",
    "    nextlink = \"https://www.boohoo.com/womens/sale?start=\"+str(1 + 60*(i-1) )+\"&sz=\"+str( 60*(i))\n",
    "    nl_response = rq.get(nextlink) # 用 requests 的 get 方法把網頁抓下來\n",
    "    soup = BeautifulSoup(nl_response.text, \"lxml\") # 指定 lxml 作為解析器\n",
    "    for url in soup.findAll('a', {'class': 'grid-tile'}):\n",
    "        response = rq.get(url.get('href')) # 用 requests 的 get 方法把網頁抓下來\n",
    "        html_doc = response.text # text 屬性就是 html 檔案\n",
    "        soup = BeautifulSoup(response.text, \"lxml\") # 指定 lxml 作為解析器\n",
    "        if soup.select('h1') != []:\n",
    "            company = soup.select('h1')[0].find('a').text\n",
    "            # 判斷是否有H1\n",
    "            if company != '' :\n",
    "                # 服務內容(有打勾)\n",
    "                pid = soup.findAll('li', {'class': 'grid-tile'})\n",
    "                Con = \",\".join([p.text.strip()  for p in pid])\n",
    "                # 地址\n",
    "                address = soup.findAll('ul', {'class': 'contacts_list'})[0].find('span', {'class': 'contacts_info'}).text\n",
    "                fp.write(company.encode('utf-8') + '='.encode('utf-8'))  \n",
    "                fp.write(Con.encode('utf-8')+ '?'.encode('utf-8')+address.encode('utf-8') +'\\n'.encode('utf-8')) \n",
    "                time.sleep(sleeptime(0,0,10))\n",
    "    print( nextlink, i)\n",
    "    i = i + 1\n",
    "    \n",
    "tEnd = time.time()#計時結束\n",
    "fp.close()\n",
    "print (\"It took %f sec\" % (tEnd - tStart))#會自動做近位\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "executionInfo": {
     "elapsed": 4133,
     "status": "error",
     "timestamp": 1620237358791,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "8N5cCmEUDZiu",
    "outputId": "28d648e5-34c7-4830-a9aa-3b26f06c5ec4"
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "# coding:utf-8\n",
    "\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "def sleeptime(hour,min,sec):\n",
    "    return hour*3600 + min*60 + sec;\n",
    "\n",
    "tStart = time.time()#計時開始\n",
    "\n",
    "fp = io.open(\"boohoo_list.txt\", \"ab+\")\n",
    "\n",
    "i = 1\n",
    "while (i<=98):\n",
    "    nextlink = \"https://www.boohoo.com/womens/sale?start=\"+str(1 + 60*(i-1) )+\"&sz=\"+str( 60*(i))\n",
    "    print(nextlink)\n",
    "    \n",
    "    nl_response = rq.get(nextlink) # 用 requests 的 get 方法把網頁抓下來\n",
    "    soup = BeautifulSoup(nl_response.text, \"lxml\") # 指定 lxml 作為解析器\n",
    "    for url in soup.findAll('a', {'class': 'name-link'}):        \n",
    "        \n",
    "        url = 'https://www.boohoo.com/' + str(url.get('href'))\n",
    "        print('url =', url)\n",
    "        \n",
    "        response = rq.get(url) # 用 requests 的 get 方法把網頁抓下來\n",
    "        html_doc = response.text # text 屬性就是 html 檔案\n",
    "        soup = BeautifulSoup(response.text, \"lxml\") # 指定 lxml 作為解析器\n",
    "        \n",
    "        name = str(soup.select('h1')[0])\n",
    "        name = name.replace('<h1 class=\"product-name is-mobile js-product-name\" itemprop=\"name\">', '')\n",
    "        name = name.replace('</h1>','')\n",
    "        print('name =', name)\n",
    "        \n",
    "        fp.write( name.encode('utf-8') + ','.encode('utf-8') ) \n",
    "        \n",
    "        date = str(datetime.datetime.now())\n",
    "        fp.write( date.encode('utf-8')+'\\n'.encode('utf-8') ) \n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        time.sleep(sleeptime(0, 0, 5))\n",
    "        \n",
    "    i = i + 1\n",
    "tEnd = time.time()#計時結束\n",
    "\n",
    "\n",
    "fp.close()\n",
    "print (\"Done! Took %f seconds\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4392,
     "status": "ok",
     "timestamp": 1620237341473,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "53bVhJ9VDZiw",
    "outputId": "71048d7c-ae40-4ab2-d769-863db313a3e7"
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "# coding:utf-8\n",
    "\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import time\n",
    "\n",
    "def sleeptime(hour,min,sec):\n",
    "    return hour*3600 + min*60 + sec;\n",
    "\n",
    "  \n",
    "tStart = time.time()#計時開始\n",
    "fp = io.open(\"marryData-List.txt\", \"ab+\")\n",
    "i = 1\n",
    "while (i<=2):\n",
    "    nextlink = \"https://www.marry.com.tw/venue-shop-kwbt2004mmir0mmpg\"+str(i)+\"mm\"\n",
    "    nl_response = rq.get(nextlink) # 用 requests 的 get 方法把網頁抓下來\n",
    "    soup = BeautifulSoup(nl_response.text, \"lxml\") # 指定 lxml 作為解析器\n",
    "    for url in soup.findAll('a', {'class': 'shop_name'}):\n",
    "        print(url)\n",
    "        \n",
    "        response = rq.get(url.get('href')) # 用 requests 的 get 方法把網頁抓下來\n",
    "        html_doc = response.text # text 屬性就是 html 檔案\n",
    "        soup = BeautifulSoup(response.text, \"lxml\") # 指定 lxml 作為解析器\n",
    "        if soup.select('h1') != []:\n",
    "            company = soup.select('h1')[0].find('a').text\n",
    "            # 判斷是否有H1\n",
    "            if company != '' :\n",
    "                # 服務內容(有打勾)\n",
    "                pid = soup.findAll('li', {'class': 'icon-check'})\n",
    "                Con = \",\".join([p.text.strip()  for p in pid])\n",
    "                # 地址\n",
    "                address = soup.findAll('ul', {'class': 'contacts_list'})[0].find('span', {'class': 'contacts_info'}).text\n",
    "                fp.write(company.encode('utf-8') + '='.encode('utf-8'))  \n",
    "                fp.write(Con.encode('utf-8')+ '?'.encode('utf-8')+address.encode('utf-8') +'\\n'.encode('utf-8')) \n",
    "                time.sleep(sleeptime(0,1,0))\n",
    "    i = i + 1\n",
    "tEnd = time.time()#計時結束\n",
    "fp.close()\n",
    "print (\"It cost %f sec\" % (tEnd - tStart))#會自動做近位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZqDcGfNDZiw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc7KYhgHmnhl"
   },
   "source": [
    "# google search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4594,
     "status": "ok",
     "timestamp": 1617686096052,
     "user": {
      "displayName": "ChingChing Lee",
      "photoUrl": "",
      "userId": "16449948928545344997"
     },
     "user_tz": -480
    },
    "id": "moo9Oe01mqF1",
    "outputId": "aec7b84c-da1e-443d-9b29-d373dcd90f6d"
   },
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "query = \"cambridge\"\n",
    "\n",
    "for j in search(query, stop=5, pause=2.0): \n",
    "\tprint(j)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Study) Web_Crowl - boohoo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
